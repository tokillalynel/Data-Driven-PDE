{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "module-mse2"
    ]
   },
   "source": [
    "# Mathematical Background of Samuel Rudy's \"Data-driven discovery of partial differential equations\"\n",
    "## Canonical form of second-order linear PDEs\n",
    "[<!-- module-mse2 badge --><span class=\"module module-mse2\">Mathematics for Scientists and Engineers 2</span>](module-mse2) \n",
    "\n",
    "```{index} Canonical form (2nd order PDE)\n",
    "```\n",
    "\n",
    "Here we consider a general second-order PDE of the function $u(x, y)$:\n",
    "\n",
    "$$ au_{xx} + bu_{xy} + cu_{yy} = f(x, y, u, u_x, u_y) $$ (eq:general)\n",
    "\n",
    "Recall from a previous notebook that the above problem is:\n",
    "\n",
    "- **elliptic** if $b^2 - 4ac > 0$\n",
    "- **parabolic** if $b^2 - 4ac = 0$\n",
    "- **hyperbolic** if $b^2 - 4ac < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any elliptic, parabolic or hyperbolic PDE can be reduced to the following **canonical forms** with a suitable coordinate transformation $\\xi = \\xi(x, y), \\qquad \\eta = \\eta(x,y)$\n",
    "\n",
    "1. Canonical form for hyperbolic PDEs: $u_{\\xi \\eta} = \\phi(\\xi, \\eta, u, u_{\\xi}, u_{\\eta}) $ or $ u_{\\xi \\xi} - u_{\\eta \\eta} = \\phi(\\xi, \\eta, u, u_{\\xi}, u_{\\eta})$\n",
    "2. Canonical form for parabolic PDEs: $u_{\\eta \\eta} = \\phi(\\xi, \\eta, u, u_{\\xi}, u_{\\eta}) $ or $ u_{\\xi \\xi} = \\phi(\\xi, \\eta, u, u_{\\xi}, u_{\\eta})$\n",
    "3. Canonical form for elliptic PDEs: $u_{\\xi \\xi} + u_{\\eta \\eta} = \\phi(\\xi, \\eta, u, u_{\\xi}, u_{\\eta})$\n",
    "\n",
    "We find the coordinate transformation\n",
    "\n",
    "$$ u_x = u_\\xi \\xi_x + u_\\eta \\eta_x, \\qquad u_y = u_\\xi \\xi_y + u_\\eta \\eta_y \\\\\n",
    "\\text{and similarly for } u_{xx}, u_{xy}, u_{yy} $$\n",
    "\n",
    "Plugging this back into {eq}`eq:general` we get\n",
    "\n",
    "$$ A u_{\\xi \\xi} + B u_{\\xi \\eta} + C u_{\\eta \\eta} = F(\\xi, \\eta, u, u_\\xi, u_\\eta) $$ (eq:general_transf)\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "& A = a(\\xi_x)^2 + b\\xi_x \\xi_y + c(\\xi_y)^2 \\\\\n",
    "& B = 2a \\xi_x \\eta_x + b(\\xi_x \\eta_y + \\xi_y \\eta_x) + 2c \\xi_y \\eta_y \\\\\n",
    "& C = a(\\eta_x)^2 + b \\eta_x \\eta_y + c(\\eta_y)^2\n",
    "\\end{aligned} $$\n",
    "\n",
    "The reader can derive this as partial differentiation practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sparse Regression with LASSO\n",
    "LASSO: Least Absolute Shrinkage And Selection Operator  \n",
    "$$ b = A x$$\n",
    "b - measurements, A - matrix, want to find x that satisfies this equations. Likely a tall A (an overdetermined system). You're not expected to find a single solution x(which might not even exist one that satisfies). More equations than unkonwns.  \n",
    "There are underdetermined systems: less equations than unknowns.  \n",
    "Example: $$ \\underbrace{\\begin{bmatrix} \\text{Person 1} \\\\ \\text{Person 2} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\end{bmatrix}}_{b}\n",
    "= \\underbrace{\\begin{bmatrix} | & | & | & | & | & | & \\dots & | \\\\ | & | & | & | & | & | & \\dots & |\\\\ Age & BP & HR & Weight & Height & Sex & \\dots & Political \\\\ | & | & | & | & | & | & \\dots & | \\\\ | & | & | & | & | & | & \\dots & |\\end{bmatrix}}_{A}\n",
    "\\underbrace{\\begin{bmatrix} o \\\\ o \\\\ \\vdots \\\\ o \\end{bmatrix}}_{x}$$\n",
    "$argmin_x L(x)$. **Least Squares** $$L = \\|Ax - b \\|_2$$\n",
    "Try to find an x value that minimises L, which is the meaning of argmin x L(x).  \n",
    "**Problem**: for least square, the $x$ will be dense, suggesting that all factors are important, but some are not. We expect some elements of $x$ to be zero.  \n",
    "**Ridge Regression(Tikhonov regularization)**\n",
    "$$ L = \\| Ax - b\\|_x + \\alpha \\| x\\|_2$$\n",
    "Assume some factors in columns in $A$ be linearly dependent(even if underdetermined), which make Least Squares very ill-conditioned.  Introduced regularization factor $\\alpha \\| x\\|_2$, penalize you if x has a deviation too large. But still give you a dense $x$.  \n",
    "**LASSO**\n",
    "$$ L = \\| Ax - b\\|_x + \\lambda \\| x\\|_1$$\n",
    "Uses a 1-norm. 1-norm promotes sparsity. It will make as many entries zero as possible. You will get a x vector that's very sparse and highlights columns of A that are most relevant. It also prevents overfitting because we only select variables that are most important. Python sklearn.linear_model.Lasso \n",
    "$\\lambda$ decides how sparse you want your vector to be. Big $\\lambda$ gives ultra-sparse vector.$\\lambda = 0$ gives the least-square solution.  \n",
    "As $\\lambda$ increases, more terms are picked up. Often we start from small lambda to big lambda, once a term dies, it can never come back. And if it accidentally kills an important terms too early it's never coming back. There are algorithms that can fix this.  \n",
    "**Elastic Net**\n",
    "$$ L = \\| Ax - b\\|_x + \\lambda \\| x\\|_1+ \\alpha \\| x\\|_2$$\n",
    "Combines both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse Identificaion of Nonlinear Dynamics(SINDy)**  \n",
    "Uses LASSO idea to discover nonlinear dynamical system.   \n",
    "Example:\n",
    "$$ \\dot x = \\sigma(y-x)$$\n",
    "$$ \\dot y = x(\\rho - z)-y$$\n",
    "$$ \\dot z = xy-\\beta z$$\n",
    "$$ \\underbrace{\\begin{bmatrix} | & | & | \\\\ | & | & | \\\\ \\dot x& \\dot y & \\dot z \\\\ | & | & | \\\\ | & | & | \\end{bmatrix}}_{\\boldsymbol{\\dot X}}\n",
    "= \\underbrace{\\begin{bmatrix} | & | & | & | & | & | & \\dots & | \\\\ | & | & | & | & | & | & \\dots & |\\\\ 1 & x & y & z & x^2 & xy & \\dots & x^5 \\\\ | & | & | & | & | & | & \\dots & | \\\\ | & | & | & | & | & | & \\dots & |\\end{bmatrix}}_{\\boldsymbol{\\Theta(X)}}\n",
    "\\underbrace{\\begin{bmatrix} | & | & | \\\\ \\xi_1 & \\xi_2 & \\xi_3 \\\\ | & | & | \\end{bmatrix}}_{\\boldsymbol{\\Xi}}$$\n",
    "$\\Theta(X)$ is a library of terms, and we want a sparse $\\Xi$ and discover the nonlinear dynamical system. Very similar to the lasso problem.  \n",
    "**Parsimonious modelling**\n",
    "$$\\| \\dot X - \\Theta(X) \\Xi\\| + \\lambda \\|\\Xi\\|_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# For PDE-FIND\n",
    "## (1) Building Libraries for Candidate Terms\n",
    "- $U \\in \\mathbb C^{mn}$: a single column vector representing data collected over $m$ time points and $n$ spatial locations.\n",
    "- $Q \\in \\mathbb C^{mn}$: additional input\n",
    "- $\\Theta(U, Q) \\in \\mathbb C^{mn \\times D}$ of D candidate linear or nonlinear terms of partial derivatives.\n",
    "$$ \\Theta(U,Q) = \\begin{bmatrix}1 & U & U^2 & \\dots & Q & U_x & U U_x & \\dots \\end{bmatrix}$$\n",
    "- $\\xi \\in \\mathbb C^D$ a column vector of coefficients of terms in the pde.\n",
    "$$ U_t = \\Theta(U, Q)\\xi$$\n",
    "Each nonzero entry in $\\xi$ represent a term in the pde, and for canonical pdes, $\\xi$ should be sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Theta(data, derivatives, derivatives_description, P, data_description = None):\n",
    "    \"\"\"\n",
    "    builds a matrix with columns representing polynoimials up to degree P of all variables\n",
    "\n",
    "    This is used when we subsample and take all the derivatives point by point or if there is an \n",
    "    extra input (Q in the paper) to put in.\n",
    "\n",
    "    input:\n",
    "        data: column 0 is U, and columns 1:end are Q\n",
    "        derivatives: a bunch of derivatives of U and maybe Q, should start with a column of ones\n",
    "        derivatives_description: description of what derivatives have been passed in\n",
    "        P: max power of polynomial function of U to be included in Theta\n",
    "\n",
    "    returns:\n",
    "        Theta = Theta(U,Q)\n",
    "        descr = description of what all the columns in Theta are\n",
    "    \"\"\"\n",
    "    \n",
    "    n,d = data.shape\n",
    "    m, d2 = derivatives.shape\n",
    "    if n != m: raise Exception('dimension error')\n",
    "    if data_description is not None: \n",
    "        if len(data_description) != d: raise Exception('data descrption error')\n",
    "    \n",
    "    # Create a list of all polynomials in d variables up to degree P\n",
    "    rhs_functions = {}\n",
    "    f = lambda x, y : np.prod(np.power(list(x), list(y)))\n",
    "    powers = []            \n",
    "    for p in range(1,P+1):\n",
    "            size = d + p - 1\n",
    "            for indices in itertools.combinations(range(size), d-1):\n",
    "                starts = [0] + [index+1 for index in indices]\n",
    "                stops = indices + (size,)\n",
    "                powers.append(tuple(map(operator.sub, stops, starts)))\n",
    "    for power in powers: rhs_functions[power] = [lambda x, y = power: f(x,y), power]\n",
    "\n",
    "    # First column of Theta is just ones.\n",
    "    Theta = np.ones((n,1), dtype=np.complex64)\n",
    "    descr = ['']\n",
    "    \n",
    "    # Add the derivaitves onto Theta\n",
    "    for D in range(1,derivatives.shape[1]):\n",
    "        Theta = np.hstack([Theta, derivatives[:,D].reshape(n,1)])\n",
    "        descr.append(derivatives_description[D])\n",
    "        \n",
    "    # Add on derivatives times polynomials\n",
    "    for D in range(derivatives.shape[1]):\n",
    "        for k in rhs_functions.keys():\n",
    "            func = rhs_functions[k][0]\n",
    "            new_column = np.zeros((n,1), dtype=np.complex64)\n",
    "            for i in range(n):\n",
    "                new_column[i] = func(data[i,:])*derivatives[i,D]\n",
    "            Theta = np.hstack([Theta, new_column])\n",
    "            if data_description is None: descr.append(str(rhs_functions[k][1]) + derivatives_description[D])\n",
    "            else:\n",
    "                function_description = ''\n",
    "                for j in range(d):\n",
    "                    if rhs_functions[k][1][j] != 0:\n",
    "                        if rhs_functions[k][1][j] == 1:\n",
    "                            function_description = function_description + data_description[j]\n",
    "                        else:\n",
    "                            function_description = function_description + data_description[j] + '^' + str(rhs_functions[k][1][j])\n",
    "                descr.append(function_description + derivatives_description[D])\n",
    "\n",
    "    return Theta, descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_linear_system(u, dt, dx, D = 3, P = 3,time_diff = 'poly',space_diff = 'poly',lam_t = None,lam_x = None, width_x = None,width_t = None, deg_x = 5,deg_t = None,sigma = 2):\n",
    "    \"\"\"\n",
    "    Constructs a large linear system to use in later regression for finding PDE.  \n",
    "    This function works when we are not subsampling the data or adding in any forcing.\n",
    "\n",
    "    Input:\n",
    "        Required:\n",
    "            u = data to be fit to a pde\n",
    "            dt = temporal grid spacing\n",
    "            dx = spatial grid spacing\n",
    "        Optional:\n",
    "            D = max derivative to include in rhs (default = 3)\n",
    "            P = max power of u to include in rhs (default = 3)\n",
    "            time_diff = method for taking time derivative\n",
    "                        options = 'poly', 'FD', 'FDconv','TV'\n",
    "                        'poly' (default) = interpolation with polynomial \n",
    "                        'FD' = standard finite differences\n",
    "                        'FDconv' = finite differences with convolutional smoothing \n",
    "                                   before and after along x-axis at each timestep\n",
    "                        'Tik' = Tikhonov (takes very long time)\n",
    "            space_diff = same as time_diff with added option, 'Fourier' = differentiation via FFT\n",
    "            lam_t = penalization for L2 norm of second time derivative\n",
    "                    only applies if time_diff = 'TV'\n",
    "                    default = 1.0/(number of timesteps)\n",
    "            lam_x = penalization for L2 norm of (n+1)st spatial derivative\n",
    "                    default = 1.0/(number of gridpoints)\n",
    "            width_x = number of points to use in polynomial interpolation for x derivatives\n",
    "                      or width of convolutional smoother in x direction if using FDconv\n",
    "            width_t = number of points to use in polynomial interpolation for t derivatives\n",
    "            deg_x = degree of polynomial to differentiate x\n",
    "            deg_t = degree of polynomial to differentiate t\n",
    "            sigma = standard deviation of gaussian smoother\n",
    "                    only applies if time_diff = 'FDconv'\n",
    "                    default = 2\n",
    "    Output:\n",
    "        ut = column vector of length u.size\n",
    "        R = matrix with ((D+1)*(P+1)) of column, each as large as ut\n",
    "        rhs_description = description of what each column in R is\n",
    "    \"\"\"\n",
    "\n",
    "    n, m = u.shape\n",
    "\n",
    "    if width_x == None: width_x = n//10\n",
    "    if width_t == None: width_t = m//10\n",
    "    if deg_t == None: deg_t = deg_x\n",
    "\n",
    "    # If we're using polynomials to take derviatives, then we toss the data around the edges.\n",
    "    if time_diff == 'poly': \n",
    "        m2 = m-2*width_t\n",
    "        offset_t = width_t\n",
    "    else: \n",
    "        m2 = m\n",
    "        offset_t = 0\n",
    "    if space_diff == 'poly': \n",
    "        n2 = n-2*width_x\n",
    "        offset_x = width_x\n",
    "    else: \n",
    "        n2 = n\n",
    "        offset_x = 0\n",
    "\n",
    "    if lam_t == None: lam_t = 1.0/m\n",
    "    if lam_x == None: lam_x = 1.0/n\n",
    "\n",
    "    ########################\n",
    "    # First take the time derivaitve for the left hand side of the equation\n",
    "    ########################\n",
    "    ut = np.zeros((n2,m2), dtype=np.complex64)\n",
    "\n",
    "    if time_diff == 'FDconv':\n",
    "        Usmooth = np.zeros((n,m), dtype=np.complex64)\n",
    "        # Smooth across x cross-sections\n",
    "        for j in range(m):\n",
    "            Usmooth[:,j] = ConvSmoother(u[:,j],width_t,sigma)\n",
    "        # Now take finite differences\n",
    "        for i in range(n2):\n",
    "            ut[i,:] = FiniteDiff(Usmooth[i + offset_x,:],dt,1)\n",
    "\n",
    "    elif time_diff == 'poly':\n",
    "        T= np.linspace(0,(m-1)*dt,m)\n",
    "        for i in range(n2):\n",
    "            ut[i,:] = PolyDiff(u[i+offset_x,:],T,diff=1,width=width_t,deg=deg_t)[:,0]\n",
    "\n",
    "    elif time_diff == 'Tik':\n",
    "        for i in range(n2):\n",
    "            ut[i,:] = TikhonovDiff(u[i + offset_x,:], dt, lam_t)\n",
    "\n",
    "    else:\n",
    "        for i in range(n2):\n",
    "            ut[i,:] = FiniteDiff(u[i + offset_x,:],dt,1)\n",
    "    \n",
    "    ut = np.reshape(ut, (n2*m2,1), order='F')\n",
    "\n",
    "    ########################\n",
    "    # Now form the rhs one column at a time, and record what each one is\n",
    "    ########################\n",
    "\n",
    "    u2 = u[offset_x:n-offset_x,offset_t:m-offset_t]\n",
    "    Theta = np.zeros((n2*m2, (D+1)*(P+1)), dtype=np.complex64)\n",
    "    ux = np.zeros((n2,m2), dtype=np.complex64)\n",
    "    rhs_description = ['' for i in range((D+1)*(P+1))]\n",
    "\n",
    "    if space_diff == 'poly': \n",
    "        Du = {}\n",
    "        for i in range(m2):\n",
    "            Du[i] = PolyDiff(u[:,i+offset_t],np.linspace(0,(n-1)*dx,n),diff=D,width=width_x,deg=deg_x)\n",
    "    if space_diff == 'Fourier': ik = 1j*np.fft.fftfreq(n)*n\n",
    "        \n",
    "    for d in range(D+1):\n",
    "\n",
    "        if d > 0:\n",
    "            for i in range(m2):\n",
    "                if space_diff == 'Tik': ux[:,i] = TikhonovDiff(u[:,i+offset_t], dx, lam_x, d=d)\n",
    "                elif space_diff == 'FDconv':\n",
    "                    Usmooth = ConvSmoother(u[:,i+offset_t],width_x,sigma)\n",
    "                    ux[:,i] = FiniteDiff(Usmooth,dx,d)\n",
    "                elif space_diff == 'FD': ux[:,i] = FiniteDiff(u[:,i+offset_t],dx,d)\n",
    "                elif space_diff == 'poly': ux[:,i] = Du[i][:,d-1]\n",
    "                elif space_diff == 'Fourier': ux[:,i] = np.fft.ifft(ik**d*np.fft.fft(ux[:,i]))\n",
    "        else: ux = np.ones((n2,m2), dtype=np.complex64) \n",
    "            \n",
    "        for p in range(P+1):\n",
    "            Theta[:, d*(P+1)+p] = np.reshape(np.multiply(ux, np.power(u2,p)), (n2*m2), order='F')\n",
    "\n",
    "            if p == 1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u'\n",
    "            elif p>1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u^' + str(p)\n",
    "            if d > 0: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+\\\n",
    "                                                   'u_{' + ''.join(['x' for _ in range(d)]) + '}'\n",
    "\n",
    "    return ut, Theta, rhs_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (2) Sparse Regression\n",
    "\n",
    "Algorithm STRidge($\\Theta$, $U_t$, $\\lambda$, tol, iters)\n",
    "\n",
    "---\n",
    "$ \\hat{\\xi} = argmin_\\xi (\\|\\Theta \\xi - U_t \\|_2^2 + \\lambda \\| \\xi\\|_2$ )# ridge regression  \n",
    "$ bigcoeffs = \\{j:|\\hat{\\xi_j} \\geq tol \\}$ # select large coefficients  \n",
    "$ \\hat{\\xi}[~bigcoeffs] = 0$  \n",
    "$ \\hat{\\xi}[bigcoeffs] = STRidge(\\Theta[:, bigcoeffs], U_t, tol, iters-1) $ # recursive call with fewer coefficients  \n",
    " return $\\hat{\\xi}$\n",
    "---\n",
    "Want to find the best predictor on the selection criteria\n",
    "$$ \\hat{\\xi} = argmin_{\\xi} \\| \\Theta(U,Q) \\xi - U_t \\|^2_2 + \\epsilon \\kappa(\\Theta(U,Q)) \\| \\xi \\|_0 $$\n",
    "- $\\kappa(\\Theta)$: condition number of matrix $\\Theta$, stronger regularization for ill-posed problems. Penalizing $\\| \\xi\\|_0$ discourages over fitting.\n",
    "- $\\| \\xi \\|_0$: the $l_0$ norm, returns the number of nonzero elements in $\\xi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    Input: X0, y\n",
    "    Return: a minimising argmin |X0 a-y|_2^2 + lam |a|_^2\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize)) # i-th column of X0.X0[:,i] has dimension n\n",
    "            X[:,i] = Mreg[i]*X0[:,i] # normalize X0: divide it by its norm.\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y),rcond=None)[0]\n",
    "    else: w = np.linalg.lstsq(X,y,rcond=None)[0]\n",
    "    num_relevant = d # number of relevant terms(= number of nonzero entries in w)\n",
    "    biginds = np.where( abs(w) > tol)[0] # extract indicies of nonzero entries\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0 # set zero to the entries of values less than tolerance level\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y),rcond=None)[0]\n",
    "        # Repeat the procedure only for the big coefficients\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y,rcond=None)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y,rcond=None)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:$X_0 \\in \\mathbb C^{n \\times d}$, $y \\in \\mathbb C^{n}$  \n",
    "Output: $w \\approx X_0^{-1}y \\in \\mathbb C^{d}$  \n",
    "If $\\lambda \\neq 0$, we still want to solve least square problem \n",
    "$$ \\|Xw - y \\|_2^2$$\n",
    "This is satisfied when \n",
    "$$ X^T y =X^T X w$$\n",
    "But we wish to add a factor $\\lambda$ penalizing a large $w$. So we want to solve\n",
    "$$ X^T y = (X^T X +\\lambda I_D)w$$\n",
    "which is another equation that cannot be solved exactly. This translates to another least square problem\n",
    "$$ \\| X^T y-(X^T X +\\lambda I_D)w\\|_2^2$$\n",
    "Hence the `if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y),rcond=None)[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    Input:  R - Theta (mn * D)\n",
    "            Ut - U_t (mn * 1)\n",
    "    Output: w_best - xi (D * 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape # mn\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol) # if above d_tol then big\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R) # Initialize epsilon = 0.001, calculate kappa\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY,rcond=None)[0]\n",
    "    # singular values are treated as zero if they are smaller than rcond times the largest singular value of a.\n",
    "    # returns w that minimises \\|Rw - Ut \\|_2\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(TrainR,TrainY,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:# 1st iteration: tol = d_tol(the initial d_tol)\n",
    "            tol = max([0,tol - 2*d_tol]) # Set tol = 0\n",
    "            d_tol  = 2*d_tol / (maxit - iter)# set d_tol = 2*d_tol / maxit = 2/25 d_tol\n",
    "            tol = tol + d_tol # tol = 0 + 2/25 d_tol = 2/25 d_tol\n",
    "            # If accuracy not improved, decrease tol: the thershold of being a big coefficient.\n",
    "            # This will incorporate more terms.\n",
    "\n",
    "    if print_best_tol: print(\"Optimal tolerance:\", tol_best)\n",
    "\n",
    "    return w_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm ElasticNet($\\Theta$, $U_t$, $\\lambda_1$,$\\lambda_2$ tol, iters)\n",
    "\n",
    "---\n",
    "$ \\hat{\\xi} = argmin_\\xi (\\|\\Theta \\xi - U_t \\|_2^2 + \\lambda_1 \\|\\xi \\|_1 + \\lambda_2 \\| \\xi\\|_2)$ # Elastic Net regression  \n",
    "$ bigcoeffs = \\{j:|\\hat{\\xi_j} \\geq tol \\}$ # select large coefficients  \n",
    "$ \\hat{\\xi}[~bigcoeffs] = 0$  \n",
    "$ \\hat{\\xi}[bigcoeffs] = ElasticNet(\\Theta[:, bigcoeffs], U_t, tol, iters-1) $ # recursive call with fewer coefficients  \n",
    " return $\\hat{\\xi}$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElasticNet(X0, Y, lam1, lam2, tol, maxit = 100,w = np.array([0]), normalize = 2):\n",
    "    \"\"\"\n",
    "    Uses accelerated proximal gradient (FISTA) to solve elastic net\n",
    "    argmin (1/2)*||Xw-Y||_2^2 + lam_1||w||_1 + (1/2)*lam_2||w||_2^2\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain size of X\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    Y = Y.reshape(n,1)\n",
    "    \n",
    "    # Create w if none is given\n",
    "    if w.size != d:\n",
    "        w = np.zeros((d,1), dtype=np.complex64)\n",
    "    w_old = np.zeros((d,1), dtype=np.complex64)\n",
    "        \n",
    "    # Initialize a few other parameters\n",
    "    converge = 0\n",
    "    objective = np.zeros((maxit,1))\n",
    "    \n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "\n",
    "    # Lipschitz constant of gradient of smooth part of loss function\n",
    "    L = np.linalg.norm(X.T.dot(X),2) + lam2\n",
    "    \n",
    "    # Now loop until converged or max iterations\n",
    "    for iters in range(0, maxit):\n",
    "         \n",
    "        # Update w\n",
    "        z = w + iters/float(iters+1)*(w - w_old)\n",
    "        w_old = w\n",
    "        z = z - (lam2*z + X.T.dot(X.dot(z)-Y))/L\n",
    "        for j in range(d): w[j] = np.multiply(np.sign(z[j]), np.max([abs(z[j])-lam1/L,0]))\n",
    "\n",
    "        # Could put in some sort of break condition based on convergence here.\n",
    "    \n",
    "    # Now that we have the sparsity pattern, used least squares.\n",
    "    biginds = np.where(w > tol)[0]\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],Y,rcond=None)[0]\n",
    "\n",
    "    # Finally, reverse the regularization so as to be able to use with raw data\n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Numerical Evaluation of Derivatives\n",
    "Proper evaluation of the numerical derivatives is the most challenging and critical task for the success of the method. This is particularly true when the discretized solution contains measurement noise. Given the wellknown accuracy problems with finite-difference approximations, we experimented with a number of more robust numerical differentiation methods.  \n",
    "The most reliable and robust method for computing derivatives from noisy data was polynomial interpolation . For each datapoint where we compute a derivative, a polynomial of degree p was fit to greater than p points and derivatives of the polynomial were taken to approximate those of the numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PolyDiff(u, x, deg = 3, diff = 1, width = 5):\n",
    "    \n",
    "    \"\"\"\n",
    "    u = values of some function\n",
    "    x = x-coordinates where values are known\n",
    "    deg = degree of polynomial to use\n",
    "    diff = maximum order derivative we want\n",
    "    width = width of window to fit to polynomial\n",
    "\n",
    "    This throws out the data close to the edges since the polynomial derivative only works\n",
    "    well when we're looking at the middle of the points fit.\n",
    "    \"\"\"\n",
    "\n",
    "    u = u.flatten()\n",
    "    x = x.flatten()\n",
    "\n",
    "    n = len(x)\n",
    "    du = np.zeros((n - 2*width,diff))\n",
    "\n",
    "    # Take the derivatives in the center of the domain\n",
    "    for j in range(width, n-width):\n",
    "\n",
    "        # Note code originally used an even number of points here.\n",
    "        # This is an oversight in the original code fixed in 2022.\n",
    "        points = np.arange(j - width, j + width + 1)\n",
    "\n",
    "        # Fit to a polynomial\n",
    "        poly = np.polynomial.chebyshev.Chebyshev.fit(x[points],u[points],deg)\n",
    "\n",
    "        # Take derivatives\n",
    "        for d in range(1,diff+1):\n",
    "            du[j-width, d-1] = poly.deriv(m=d)(x[j])\n",
    "\n",
    "    return du\n",
    "\n",
    "def PolyDiffPoint(u, x, deg = 3, diff = 1, index = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Same as above but now just looking at a single point\n",
    "\n",
    "    u = values of some function\n",
    "    x = x-coordinates where values are known\n",
    "    deg = degree of polynomial to use\n",
    "    diff = maximum order derivative we want\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(x)\n",
    "    if index == None: index = (n-1)//2\n",
    "\n",
    "    # Fit to a polynomial\n",
    "    poly = np.polynomial.chebyshev.Chebyshev.fit(x,u,deg)\n",
    "    \n",
    "    # Take derivatives\n",
    "    derivatives = []\n",
    "    for d in range(1,diff+1):\n",
    "        derivatives.append(poly.deriv(m=d)(x[index]))\n",
    "        \n",
    "    return derivatives\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
